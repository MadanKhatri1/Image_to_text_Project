import streamlit as st
from transformers import pipeline, ViTImageProcessor, GPT2Tokenizer, VisionEncoderDecoderModel
from PIL import Image

# Load the model, processor, and tokenizer
model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
processor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
tokenizer = GPT2Tokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

# Define the image captioning pipeline
image_captioning_pipeline = pipeline("image-to-text", model=model, tokenizer=tokenizer, feature_extractor=processor)

def generate_caption(image):
    caption = image_captioning_pipeline(image, max_new_tokens=50)
    return caption[0]['generated_text']

# Streamlit app
st.title("Image Captioning with Transformers")
st.write("Upload an image and get a caption generated by a transformer model.")

uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    image = Image.open(uploaded_file)
    st.image(image, caption='Uploaded Image.', use_column_width=True)
    st.write("")
    st.write("Generating caption...")
    
    caption = generate_caption(image)
    st.write(f"Caption: {caption}")




